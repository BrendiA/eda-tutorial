---
title: "ETC5521 Tutorial 4"
subtitle: "Working with a single variable, making transformations, detecting outliers, using robust statistics"
author: "Dr Michael Lydeamore (initially prepared by Dr Emi Tanaka)"
date: "Week 4"
output:
  html_document:
    toc: true
---

# üèÅ Objectives

These are exercises in making plots of one variable and what can be learned about the distributions and the data patterns and problems.

# üèãüèº Preparation 

Install the following R-packages if you do not have it already:

```
install.packages(c("ggplot2movies", "lawstat", "mi", "HSAUR2", "coin", "DAAG", "vcd", "vcdExtra", "bayesm", "AER", "flexmix", "psych"))
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(colorspace)
library(patchwork)
library(scales)

select <- dplyr::select 

# Set default theme for document
ggplot2::theme_set(theme_bw())
```

# Introduction

In the chat window, say hello, and if you feel comfortable tell us something fun about yourself, or what you have done this last week.

The following are questions based from Unwin (2015) Graphical Data Analysis with R Chapter 3-4. For every data you should perform an IDA to understand the data structure, e.g. find the number of observations, the variables in the data, and if there are any missing values.

# Exercise 1: Galaxies

Load the `galaxies` data in the `MASS` package and answer the following questions based on this dataset.

```{r}
data(galaxies, package = "MASS")
help(galaxies, package = "MASS") # documentation
```

You can access documentation of the data (if available) using the `help` function specifying the package name in the argument. 

```{r, eval = FALSE}
# Fix typo (see documentation)
galaxies[78] <- 26960 
```


## a.

What does the data contain? And what is the data source?

* The data contains velocities in km/sec of `r length(galaxies)` galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. 
* The original data is from Postman et al. (1986) and this data is from Roeder with 83rd observation removed from the original dataset as well as typo for the 78th observation.

Postman, M., Huchra, J. P. and Geller, M. J. (1986) Probes of large-scale structures in the Corona Borealis region. Astronomical Journal 92, 1238‚Äì1247  

Roeder, K. (1990) Density estimation with confidence sets exemplified by superclusters and voids in galaxies. Journal of the American Statistical Association 85, 617‚Äì624.

## b.

How many observations are there?

There are `r length(galaxies)` observations.

## c 

Draw a histogram, a boxplot and a density plot for this data. Experiment with different bin widths for the histogram and density plot.

```{r}
# Create ggplot object as base
p <- ggplot(tibble(velocity = galaxies), 
            aes(x = velocity))

p1 <- p + geom_histogram(binwidth = 1000, colour = "white")
p2 <- p + geom_boxplot()
p3 <- p + geom_density()

# Combine plots
(p1 / p2 & theme(axis.text.x = element_blank(),
                 axis.ticks.x = element_blank(),
                 axis.title = element_blank())) /
  p3
```

* `binwidth` controls the width of the bin on the x-axis
  * *e.g.* binwidth = 10 means (10 - 20], (20, 30], (30, 40] ...

## d.

What is the primary information that you learn from the plots?

* Most galaxies have a velocity of ~ 21,000 km/sec
* There are some galaxies which are much faster/slower than other galaxies
  * some reach up ~ 34,500 km/h
  * some are only at ~ 10,000 km/h

# Exercise 2: Student survey

The `survey` data contains the responses of 237 Statistics I students at the University of Adelaide to a number of questions. Load the data `survey` in the `MASS` package and answer the following questions. 

## a. 

What variables are in the data?

```{r}
data(survey, package = "MASS")
glimpse(survey)
```

The variables in the data are: `r paste0("<code>", colnames(survey), "</code>", collapse = ", ")`

## b. 

Perform some initial data analysis. Are there anything you notice? 

```{r}
visdat::vis_miss(survey)
```

* There are noticeable missing data structure where there are fair bit of pulse and height data missing.
* The `M.I` variable is an indicator of whether the height was recorded using metric or imperial units so naturally missing when `Height` is missing.

## c & d.

(c) Draw a histogram of student heights and overlay a density plot of the data. Is there evidence of bimodality? 

(d) Experiment with different binwidths for the histogram and different bandwidths for the density plot. Which choices do you think are best for conveying the information in the data?

```{r}
ggplot(survey, aes(x = Height, y = ..density..)) +
  geom_histogram(color = "white", binwidth = 2) +
  geom_density(color = "red")
```

## e. 

Compare heights of male and female students using separate density plots that are common scaled and aligned with one another.

```{r}
survey %>% 
  filter(!is.na(Sex)) %>% 
  ggplot(aes(x = Height, fill = Sex)) +
  geom_density() +
  facet_wrap(~ Sex, ncol = 1) +
  scale_fill_discrete_qualitative()
```


# Exercise 3: Movie lengths

Load the `movies` dataset in the `ggplot2movies` package and answer the following questions based on it.

```{r}
data("movies", package = "ggplot2movies")
glimpse(movies)
```

## a. 

How many observations are in the data?

There are `r nrow(movies)` observations.

## b. 

Draw a histogram with an appropriate bin width that shows the peaks at 7 minutes and 90 minutes. Draw another set of histograms to show whether these peaks existed both before and after 1980.

```{r}
movies %>%
  # movies before/after 1980
  mutate(after1980 = if_else(year > 1980, "after 1980", "1980 and before")) %>% 
  # bind new data, with after1980 = "all" to facilitate facet_wrap
  bind_rows(mutate(.data = movies, after1980 = "all")) %>%
  ggplot() +
  geom_histogram(aes(x = length), 
                 binwidth = 1,
                 colour = "black", 
                 fill = "yellow") +
  scale_x_continuous(breaks = c(0, 7, seq(30, 180, 30)),
                     limits = c(0, 180)) +
  facet_wrap(~ after1980,
             ncol = 1)
```

* `binwidth`: controls the width of each bin on the x-axis
  * bins = 10 means (0,10], (10, 20], (20,30]
* `bins`: number of bins or bars on the histogram
  * if your range of the data is 0 - 100
  * bins = 5 means (0, 5], (5, 10]...

## c. 

The variable `Short` indicates whether the film was classified as a short film (`1`) or not (`0`). Draw plots to investigate what rules was used to define a film as "short" and whether the films have been consistently classified. 

```{r}
movies %>% 
  group_by(Short) %>% 
  summarise(x = list(summary(length))) %>% 
  unnest_wider(x)
```

* The maximum length for a film classified as short is `r filter(movies, Short == 1) %>% pull(length) %>% max()` minutes.

```{r q3plot2}
movies %>%
  # convert to factor
  mutate(short = factor(Short, labels = c("Long", "Short"))) %>%
  ggplot(aes(x = length, y = short)) +
  geom_violin(aes(fill = short)) +
  geom_boxplot(width = 0.05, colour = "grey", outlier.colour = "black", outlier.size = 1) +
  # log transformation
  scale_x_log10(limits = c(1, 240),
                breaks = c(1, 7, 10, 15, 20, 30, 45, 50, 70, 90, 110, 240)) +
  scale_fill_discrete_qualitative() +
  labs(y = "",
       x = "log minutes")
```

# Exercise 4: Zuni educational funding 

The `zuni` dataset in the `lawstat` package contains information on 89 school districts in New Mexico, U.S.A., for three variables: the district name, the average revenue per student in dollars, and the number of students. This data is from a court case that raised concern whether the expenditures per student was approximately equal in the state. 

```{r q4data}
data(zuni, package = "lawstat")
glimpse(zuni)
```

## a. 

Choose a graphic that help answer the question: are the lower and highest 5% of the revenue values extreme?

```{r q4plot}
df4 <- zuni %>%
  mutate(extreme = case_when(
    Revenue <= quantile(Revenue, 0.05) ~ "lower 5%",
    Revenue >= quantile(Revenue, 0.95) ~ "upper 5%",
    TRUE ~ "Majority"
  ))

df4 %>% 
  ggplot(aes(x = "", y = Revenue)) +
  ggbeeswarm::geom_beeswarm(aes(color = extreme)) +
  coord_flip() +
  scale_color_discrete_qualitative()
```


## b. 

Calculate the sample mean, trimmed mean and Winsorised mean of average revenue per student. For trimmed mean and Winsorized mean, trim the 5% of lowest and highest values. Are your results expected? Hint check the help for the `mean` function and also check out `winsor.mean` in the `psych` package.

```{r}
ggplot(df4) +
  geom_histogram(aes(x = Revenue)) +
  # Mean, with 5% of lowest/highest values
  geom_vline(xintercept = mean(df4$Revenue), colour = "red") +
  # Mean, removed 5% of lowest/highest values
  geom_vline(xintercept = mean(zuni$Revenue, trim = 0.05), colour = "blue") 

# Median as a measure of location, seems like a better approach here
ggplot(df4) +
  geom_histogram(aes(x = Revenue)) +
  geom_vline(xintercept = median(df4$Revenue), colour = "green") 
```

* Sample mean is `r label_dollar()(mean(zuni$Revenue))`
  * Sample mean is not a robust measure of location
  
* Trimmed mean is `r label_dollar()(mean(zuni$Revenue, trim = 0.05))`
  * *e.g.* discards outliers
  * can be controversial, because we have to decide % observations to discard

* Winsorised mean is `r label_dollar()(psych::winsor.mean(zuni$Revenue, trim = 0.05))`. 
  * The lower estimate of the location using trimmed and Winsorised mean compared to the sample mean is expected due to the extreme values on the upper tail. 
  
## c. 

Draw a density plot after removing the lowest and highest 5% of the values. Does the distribution of the remaining values look symmetric? What do you conclude after looking at the Q-Q plot?

```{r}
df4 <- zuni %>%
  filter(between(Revenue,
                 quantile(Revenue, 0.05),
                 quantile(Revenue, 0.95)))

ggplot(data = df4) +
  geom_density(aes(x = Revenue))

ggplot(df4, aes(sample = Revenue)) +
  geom_qq() +
  geom_qq_line(color = "red")
```


# Exercise 5: Multiple sclerosis

Load the dataset `MSPatients` in `vcd` package and produce graphics that will be best to answer the following questions: 



```{r}
# Read-in data
data("MSPatients", package = "vcd")
```

* Multiple sclerosis (MS) is a potentially disabling disease of the brain and the central nervous system
  * might lose ability to feel certain parts of the body or lose control of parts of body entirely
  * AKA virtually invisible disease
  
* 3D Array 
  * 1D array is a vector
  * 2D array is a data frame or a matrix
  
```{r}
# Long format for plotting
df5 <- 
  # convert array to data frame
  as_tibble(MSPatients) %>%
  pivot_longer(everything(),
    names_to = c("rating", "neurologist"),
    names_pattern = "(.*)\\.(.*)",
    values_to = "count"
  ) 

# Relevel rating
df5 <- df5 %>% 
  mutate(rating = fct_relevel(rating,
                              levels = c("Doubtful",
                                         "Possible", 
                                         "Probable", 
                                         "Certain")))

df5 <- df5 %>% 
  group_by(rating, neurologist) %>% 
  summarise(count = sum(count))
```

* `()`: capturing groups
* `\\.`: separated by a full stop
* `.`: wild meta character - match any character except a new line ("\n")
* `*`: quantifier - 0 or more instances

## a. 

How do the distributions of the ratings of the neurologists compare?

```{r}
ggplot(df5) +
  geom_col(aes(x = neurologist,
               y = count,
               fill = rating),
           position = "fill",
           colour = "black"
           ) +
  scale_fill_discrete_sequential()
```

* There are far more patients in Winnipeg than New Orleans so comparing between neurologists, it's not appropriate compare the raw counts. 
  * percent stacked bar chart is therefore used

```{r}
df5 %>% 
  group_by(neurologist) %>% 
  summarise(total = sum(count))
```

## b. 

How would you describe their rating patterns?

* Either the Winnipeg neurologist is much more likely to rate their patient as certain for the diagnosis of multiple sclerosis *OR*
* Winnipeg neurologist has more patients that display clear symptoms of multiple sclerosis. 

# Exercise 6: Occupational mobility 

Load the dataset `Yamaguchi87` in `vcdExtra` package and produce graphical or numerical summaries to answer the following questions.

```{r}
data(Yamaguchi87, package = "vcdExtra")
```

From the data documentation:

* Five status categories
    * Non-manuals
        * upper non-manuals(`UpNM`): professionals, managers & officials
        * lower non-manuals(`LoNM`): proprietors, sales workers
    * Manuals
        * upper manuals(`UpM`): skilled workers
        * lower manuals(`LoM`): are semi-skilled & unskilled non-form workers
    * Farm(`Farm`): farmers & farm labourers

## a.

How do the distributions of occupations of the sons in the three countries compare?

```{r}
Yamaguchi87 %>% 
  group_by(Country, Son) %>% 
  summarise(Freq = sum(Freq)) %>% 
  ggplot() +
  geom_col(aes(x = Country, y = Freq, fill = Son),
           colour = "black") +
  scale_fill_discrete_sequential(palette = "Terrain 2") 
```

## b. 

How do the distributions of the sons' and fathers' occupations in the UK compare?

```{r}
Yamaguchi87 %>% 
  filter(Country == "UK") %>% 
  pivot_longer(cols = Son:Father,
               names_to = "person",
               values_to = "category") %>% 
  group_by(person, category) %>% 
  # compute frequency
  summarise(Freq = sum(Freq)) %>% 
  ggplot() +
  geom_col(aes(x = category, y = Freq, fill = person),
           colour = "black", 
           position = "dodge"
           ) +
  scale_fill_discrete_qualitative()
```

## c. 

Are you surprised by the results or are they what you would have expected?

* There are more farmer workers in the older generation and more upper non-manual workers in the new generation. 
  * This is perhaps somewhat expected as the technology improves, more farm work may be replaced with machines and there may be more opportunities in non-manual work.

# Exercise 7: Whisky

Load the data `Scotch` in `bayesm` package and consider the following questions. 

```{r}
data("Scotch", package = "bayesm")

# Long form for plotting
df7 <- Scotch %>%
  pivot_longer(everything(),
    names_to = "brand",
    values_to = "consumed") %>%
  filter(consumed == 1) %>%
  group_by(brand) %>%
  summarise(count = n())

# Clean names to match flexmix::whiskey_brands
df7 <- df7 %>% 
  mutate(
    brand = str_replace_all(brand, "\\.", " "), # replace "." with a space
    brand = str_squish(brand), # remove excess white spaces
    brand = case_when(brand == "the Singleton" ~ "Singleton",
                      brand == "Grants" ~ "Grant's",
                      brand == "Black White" ~ "Black & White",
                      brand == "Pinch Haig " ~ "Pinch (Haig)",
                      brand == "J B" ~ "J&B",
                      brand == "Dewar s White Label" ~ "Dewar's White Label",
                      brand == "Other Brands" ~ "Other brands",
                      TRUE ~ brand)
    )
```

* Brands used for those respondents who report consuming scotch
  * `1` consumed in the last year, `0` if not

## a.

Produce a bar plot of the number of respondents per brand. What ordering of the brands do you think is the best?

```{r}
df7 %>% 
  mutate(brand = fct_reorder(brand, count)) %>% 
  ggplot() +
  geom_col(aes(x = count, y = brand)) 
```


## b.

There are 20 named brands and one category that is labelled as `Other.brands`. Produce a barplot that you think best reduces the number of categories by selecting a criteria to lump certain brands to the `Other` category. 

```{r}
df7 %>%
  mutate(
    brand = if_else(count > 200, brand, "Other brands"),
    brand = fct_reorder(brand, count, .fun = min)
  ) %>%
  ggplot(aes(count, brand)) +
  geom_col()
```


## c.

The data `whiskey_brands` in the `flexmix` package has the information on whether the whiskey is a single malt or blend. This data is loaded when you load the `whiskey` data in the same package:

```{r}
# load whiskey_brands data
data(whiskey, package = "flexmix")
```

How would you incorporate this information on your graphics? 

```{r}
df7 <- df7 %>% 
  left_join(
    janitor::clean_names(whiskey_brands), # clean names to facilitate join
    by = "brand") 

# Fill by type or bottled
df7 %>% 
  mutate(brand = fct_reorder(brand, count)) %>% 
  ggplot() +
  geom_col(aes(x = count, y = brand, fill = type)) +
  scale_fill_discrete_qualitative(na.value = "grey80")
```



